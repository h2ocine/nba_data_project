{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kadem\\AppData\\Local\\Temp\\ipykernel_77888\\1462801910.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import utils\n",
    "import ml_functions\n",
    "\n",
    "red_code = '\\033[91m'\n",
    "blue_code = '\\033[94m'\n",
    "green_code = '\\033[92m'\n",
    "yellow_code = '\\033[93m'\n",
    "reset_code = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# word2vec_model_path = \"models/modele_word2vec.bin\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# word2vec_model = Word2Vec.load_word2vec_format(word2vec_model_path, binary=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m glove_vectors_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/vecteurs_glove.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m glove_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglove_vectors_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:2059\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2058\u001b[0m     header \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(fin\u001b[38;5;241m.\u001b[39mreadline(), encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header\u001b[38;5;241m.\u001b[39msplit()]  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(vocab_size, limit)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# word2vec_model_path = \"models/modele_word2vec.bin\"\n",
    "# word2vec_model = Word2Vec.load_word2vec_format(word2vec_model_path, binary=True)\n",
    "\n",
    "glove_vectors_path = \"models/vecteurs_glove.txt\"\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(glove_vectors_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data & Préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/movies/movies1000/\"\n",
    "alltxts,alllabs = utils.load_movies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.DataFrame()\n",
    "movies_df['text'] = alltxts\n",
    "movies_df['label'] = alllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_alltxts = [utils.preprocess(alltxt) for alltxt in movies_df.text]\n",
    "\n",
    "preprocessed_movies_df = pd.DataFrame()\n",
    "preprocessed_movies_df['text'] = preprocessed_alltxts\n",
    "preprocessed_movies_df['label'] = alllabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(analyze_function, **vectorizer_args):\n",
    "\n",
    "    print(f'{yellow_code}count{reset_code}')\n",
    "    ml_functions.count_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    print(f'\\n{yellow_code}tfidf{reset_code}')\n",
    "    ml_functions.tfidf_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    print(f'\\n{yellow_code}hasing{reset_code}')\n",
    "    ml_functions.hashing_analyze(movies_df, analyze_function, **vectorizer_args)\n",
    "\n",
    "    # print(f'\\n{yellow_code}word2vec{reset_code}')\n",
    "    # ml_functions.word2vec_analyze(movies_df, analyze_function, word2vec_model_path)\n",
    "\n",
    "    # print(f'\\n{yellow_code}glove{reset_code}')\n",
    "    # ml_functions.glove_analyze(movies_df, analyze_function, glove_vectors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_evaluations(analyze_function):\n",
    "\n",
    "    print(f'{blue_code}Non binary{reset_code}')\n",
    "    evaluation(analyze_function)\n",
    "\n",
    "    print(f'{blue_code}Binary{reset_code}')\n",
    "    evaluation(analyze_function, binary=True)\n",
    "\n",
    "    print(f'{blue_code}Stop word{reset_code}')\n",
    "    evaluation(analyze_function, stop_words='english')\n",
    "\n",
    "    print(f'{blue_code}Réduction du dictionnaire{reset_code}')\n",
    "    evaluation(analyze_function, max_df=.75)\n",
    "\n",
    "    print(f'{blue_code}Bigram{reset_code}')\n",
    "    evaluation(analyze_function,ngram_range=(1, 2))\n",
    "\n",
    "    print(f'{blue_code}Trigram{reset_code}')\n",
    "    evaluation(analyze_function, ngram_range=(1, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régresionn linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8513853904282116\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9217230430760769\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8413461538461539\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.915972899322483\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7775\u001b[0m\n",
      "\u001b[92mF1 score :\t0.789598108747045\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8528963224080602\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.logistic_regression_analyze\n",
    "\n",
    "all_evaluations(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7475\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7292225201072386\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8470961774044351\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.855036855036855\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9228980724518114\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.81\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8173076923076923\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8954723868096702\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.svm_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.665\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6666666666666666\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.6649916247906197\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.63\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6390243902439025\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.6298907472686818\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.6525\u001b[0m\n",
      "\u001b[92mF1 score :\t0.6445012787723785\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.652628815720393\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.decision_tree_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mcount\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7875\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7858942065491183\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8660341508537714\u001b[0m\n",
      "\n",
      "\u001b[93mtfidf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7905759162303665\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8813220330508262\u001b[0m\n",
      "\n",
      "\u001b[93mhasing\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.77\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7688442211055276\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8677716942923572\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "analyze_function = ml_functions.random_forest_analyze\n",
    "\n",
    "evaluation(analyze_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régresionn linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ml_functions' has no attribute 'count_analyze_logistic_regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myellow_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mwithout preprocessing vectorizer Count\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreset_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mml_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_analyze_logistic_regression\u001b[49m(movies_df)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ml_functions' has no attribute 'count_analyze_logistic_regression'"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8413461538461539\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.915972899322483\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.tfidf_analyze_logistic_regression(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count Bi-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8625\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8635235732009926\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9302482562064052\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count Bi-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwithout preprocessing vectorizer Count Tri-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.75\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7237569060773481\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8317207930198256\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}without preprocessing vectorizer Count Tri-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(movies_df,ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.835\u001b[0m\n",
      "\u001b[92mF1 score :\t0.835\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9159978999474987\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer TfIdf\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.8325\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8385542168674699\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.9093977349433736\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer TfIdf{reset_code}')\n",
    "ml_functions.tfidf_analyze_logistic_regression(preprocessed_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer Count Bi-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.81\u001b[0m\n",
      "\u001b[92mF1 score :\t0.8118811881188119\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8959973999349984\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer Count Bi-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df,ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mwith preprocessing vectorizer Count Tri-gram\u001b[0m\n",
      "\u001b[92mAccuracy :\t0.7625\u001b[0m\n",
      "\u001b[92mF1 score :\t0.7425474254742548\u001b[0m\n",
      "\u001b[92mAUC :\t\t0.8332208305207631\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f'{yellow_code}with preprocessing vectorizer Count Tri-gram{reset_code}')\n",
    "ml_functions.count_analyze_logistic_regression(preprocessed_movies_df,ngram_range=(3,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
